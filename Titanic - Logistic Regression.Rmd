---
title: "Titanic - Logistic Regression"
author: "Mario Durantez + Andres Garcia's edits"
date: "08 de octubre de 2017"
output:
  html_document: default
  pdf_document: default
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```
```{r echo = FALSE}
## Supress warning messages
oldw <- getOption("warn")
options(warn = -1)
```


# Intro to Logistic Regression Analysis

The predictors can be continuous, categorical or a mix of both.

Binomial logistic regression = the variable to predict is binary.

To check how R is going to deal with the categorical variables, we can use contrasts() to see how the variables have been dummyfied and how to intepret them in a model.


# Load clean training database
```{r}
df <- read.csv('./train_cleaned.csv')
summary(df)
```


### Some variables are not useful for our model, so we keep only the relevant variables
```{r}
db <- subset(df, select=c("Survived","Pclass","Sex","Age","SibSp","Parch","Fare","FamilySize","Child", "Title"))
```

###Slice the dataset into a train and a test dataset
```{r}
train <- db[1:800,]
test <- db[801:891,]
```

#Create the model
```{r}
model2 <- glm(Survived ~., family=binomial(link='logit'), data=train) # (the parameter family=binomial selects the logistic regression)
summary(model2)
```
# Interpret the results of the logistic regression model

If a variable has three asterisks (***) --> the variable is statistically significant.

If estimate is negative (ie sexmale) --> All the other variables being equal, a male passenger is less likely to have survived.

For instance, if the estimate for sexmale is -2.5137 --> Being male reduces the likelihood of surviving by 2.5137
```{r}
exp(coef(model2)) # To see the odds ratio coefficients
```

###Some variables are not statistically significant, and bloat the model, so we are removing them from it and repeating the process (i.e.: title)
```{r}
db <- subset(db, select=c(1,2,3,4,5,6,7,8,9))
train <- db[1:800,]
test <- db[801:891,]
model <- glm(Survived ~., family=binomial(link='logit'), data=train)
summary(model)
exp(coef(model))
```

#Analysis of variance
The difference between the null deviance and the residual deviance shows how our model is doing against the null model (a model with only the intercept).

Higher reductions of Residual Deviance mean that the variable is more valuable for the Logistic Regression
In this case, PClass , Sex and Age are the most valuable.

We see that the model just with the intercept has a resid. Dev of 1066.33, whereas the model with Pclass has a resid. Dev of 981.56
The three asterisks (***) show that the variable Pclass improved the model significantly (the extra variance it explains with the variable Pclass is significant).
```{r}
anova(model, test="Chisq")
```


#Assessing predictive ability of the model
###Make a prediction and compare results against the test dataset
```{r}
fitted.results <- predict(model,newdata=subset(test,select=c(2,3,4,5,6,7,8,9)),type='response')
## In newdata you have to include a selection of the columns that are part of your model (your independent variables)
fitted.results <- ifelse(fitted.results > 0.5,1,0)
## the output of predict is a value in range 0 to 1. If the probability is greater than 0.5 then we give it a 1, otherwise we assign 0 to that value
misClasificError <- mean(fitted.results != test$Survived)
print(paste('Accuracy',1-misClasificError))
```
###Compare prediction accuracy vs accuracy for train set to avoid overfitting
To rule out overfitting problems, it is useful to compare the accuracy of the model on both data sets.
If the accuracy for the test set is significantly lower than the accuracy for the training set, there might be an overfitting problem. The model is too rigid to predict sets other than the training set.
```{r include = FALSE}
fitted2.results <- predict(model,newdata=subset(train,select=c(2,3,4,5,6,7,8,9)),type='response')
## In newdata you have to include a selection of the columns that are part of your model (your independent variables)
fitted2.results <- ifelse(fitted2.results > 0.5,1,0)
## the output of predict is a value in range 0 to 1. If the probability is greater than 0.5 then we give it a 1, otherwise we assign 0 to that value
misClasificError2 <- mean(fitted2.results != train$Survived)
```
```{r echo = FALSE}
print(paste('Accuracy for training set',1-misClasificError2))
```
Errors have close values, meaning there is no overfitting.

### Learning curve
```{r}
m <- dim(train)[1]

error_test = c()
error_train = c()
train_probs = c()
test_probs = c()

for (i in 10:m) {
  ##Create model from first i training examples
  temp_model <- glm(Survived ~., family=binomial(link='logit'), data=train[1:i,])
  
  ##Check accuracy for test and train sets
  temp_test.results <- predict(temp_model,newdata=subset(test,select=c(2,3,4,5,6,7,8,9)),type='response')
  test_probs <- temp_test.results
  temp_test.results <- ifelse(temp_test.results > 0.5,1,0)
  error_test[i] <- mean(temp_test.results != test$Survived)
  
  temp_train.results <- predict(temp_model,newdata=subset(train[1:i,],select=c(2,3,4,5,6,7,8,9)),type='response')
  train_probs <- temp_train.results
  temp_train.results <- ifelse(temp_train.results > 0.5,1,0)
  error_train[i] <- mean(temp_train.results != train[1:i,]$Survived)
}

## Save raw probability of survival for visualization purposes
combined_probs = c(train_probs, test_probs)
df_probs <- data.frame(df$PassengerId,combined_probs)
names(df_probs) <- c("PassengerId", "Survival probability")
write.csv(df_probs, file="survival_probabilities_lr.csv", row.names = FALSE)

## Plot Results
plot(1:m, error_test, type='l', col='green', ylim=c(0,0.5), xlab='Training examples', ylab='error %', main='Learning curve')
lines(1:m, error_train, col='red')
legend("topright", inset=.05, title="Data set", c("Train", "Test"), fill=c('red', 'green'), horiz=TRUE)

```

#Non-linear regression
Linear regression solutions are always straight lines. Using an n-th grade curve insted would allow for the regression to create a more flexible solution that fits the problem better.
We will study whether this problem can be better fit by 2nd grade curves

First, new, second grade features are needed. Only numeric, significant features will be used for this. (Age and sibsp)
```{r}
gradetwo_train <- train
gradetwo_train$age_age <- gradetwo_train$Age * gradetwo_train$Age
gradetwo_train$age_sibsp <- gradetwo_train$Age * gradetwo_train$SibSp
gradetwo_train$sibsp_sibsp <- gradetwo_train$SibSp * gradetwo_train$SibSp

gradetwo_test <- subset(test,select=c(2,3,4,5,6,7,8,9))
gradetwo_test$age_age <- gradetwo_test$Age * gradetwo_test$Age
gradetwo_test$age_sibsp <- gradetwo_test$Age * gradetwo_test$SibSp
gradetwo_test$sibsp_sibsp <- gradetwo_test$SibSp * gradetwo_test$SibSp
```

Create a 2nd grade model and test it's accuracy
```{r}
gradetwo_model <- glm(Survived ~., family=binomial(link='logit'), data=gradetwo_train)

gradetwo_fitted.results <- predict(gradetwo_model,newdata=gradetwo_train,type='response')
gradetwo_fitted.results <- ifelse(gradetwo_fitted.results > 0.5,1,0)
gradetwo_misClasificError <- mean(gradetwo_fitted.results != train$Survived)
print(paste('Accuracy on train set',1-gradetwo_misClasificError))

gradetwo_fitted.results <- predict(gradetwo_model,newdata=gradetwo_test,type='response')
gradetwo_fitted.results <- ifelse(gradetwo_fitted.results > 0.5,1,0)
gradetwo_misClasificError <- mean(gradetwo_fitted.results != test$Survived)
print(paste('Accuracy on test set',1-gradetwo_misClasificError))
```
Model accuracy has improved slightly from the original model thanks to second grade curves.
One of the bigger problems of this technique for this particular dataset is that most of the more significant features are not numerical. It will be interesting to see how this would work on a more floating-point oriented database, and also how this can lead to an overfitting issue.

#ROC
The ROC curve represents the True positive versus false positive rate
A better model has a bigger area under this curve
```{r include = FALSE}
#library(ROCR)
```
```{r}
#p <- predict(model, newdata=subset(test,select=c(2,3,4,5,6,7,8,9)), type="response")
#pr <- prediction(p, test$Survived)
#prf <- performance(pr, measure = "tpr", x.measure = "fpr")
#plot(prf)

#auc <- performance(pr, measure = "auc")
#auc <- auc@y.values[[1]]
#auc
```

```{r echo = FALSE}
## Restore warning messages
options(warn = oldw)
```


